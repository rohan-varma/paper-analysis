{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[Link](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) to paper. \n",
    "\n",
    "Some insights from [this paper](https://arxiv.org/pdf/1301.3781.pdf) were also used.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Word2Vec model has become a standard method for representing words as dense vectors. This is typically done as a preprocessing step, after which the learned vectors are fed into a discriminative model (typically an RNN) to generate predictions such as movie review sentiment, do machine translation, or even generate text, character by character. \n",
    "\n",
    "Previously, the bag of words model was commonly used to represent words and sentences as numerical vectors, which could then be fed into a classifier (for example Naive Bayes) to produce output predictions. This model would represent documents as vectors at which index $i$ is zero if the $ith$ word does not exist in the vocabulary, otherwise it is the number of times that word occured in that particular document. \n",
    "\n",
    "This model represented words as atomic units, assuming that all words were independent of each other. It had success in several fields such as document classification, spam detection, and even sentiment analysis, but its assumptions (that words are completely independent of each other) were too strong for more powerful and accurate models. Enter Word2Vec. \n",
    "\n",
    "### The Skip-Gram and Continuous Bag of Words Models\n",
    "\n",
    "There are actually two different implementations of models that learn dense representation of words: the Skip-Gram model and the Continuous Bag of Words model. Both of these models learn dense vector representation of words, based on the words that surround them (ie, their *context*). \n",
    "\n",
    "The difference is that the skip-gram model predicts context (surrounding) words given the current word, wheras the continuous bag of words model predicts the current word based on several surrounding words. \n",
    "\n",
    "This notion of \"surrounding\" words is best described by considering a center (or current) word and a window of words around it. For example, if we consider the sentence \"The quick brown fox jumped over the lazy dog\", and a window size of 2, we'd have the following pairs for the skip-gram model:\n",
    "\n",
    "![img](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "In contrast, for the CBOW model, we'll input the context words within the window (such as \"the\", \"brown\", \"fox\") and aim to predict the target word \"quick\" (simply reversing the input -> prediction pipeline from the skip-gram model). \n",
    "\n",
    "The following is a visualization of the skip-gram and CBOW models:\n",
    "\n",
    "In this [paper], the overall recommendation was to use the skip-gram model, since it had been shown to perform better on analogy-related tasks than the CBOW model. Overall, if you understand one model, it is pretty easy to understand the other: just reverse the inputs and predictions. Since both papers focused on the skip-gram model, this post will do the same. \n",
    "\n",
    "### Learning with the Skip-Gram Model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
