{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](https://raw.githubusercontent.com/rohan-varma/paper-analysis/master/word2vec-papers/models.png)\n",
    "[Link](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) to paper. \n",
    "\n",
    "Some insights from [this paper](https://arxiv.org/pdf/1301.3781.pdf) were also used.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Word2Vec model has become a standard method for representing words as dense vectors. This is typically done as a preprocessing step, after which the learned vectors are fed into a discriminative model (typically an RNN) to generate predictions such as movie review sentiment, do machine translation, or even generate text, character by character (TODO link to char-rnn). \n",
    "\n",
    "Previously, the bag of words model was commonly used to represent words and sentences as numerical vectors, which could then be fed into a classifier (for example Naive Bayes) to produce output predictions. This model would represent documents as vectors at which index $i$ is zero if the $ith$ word does not exist in the vocabulary, otherwise it is the number of times that word occured in that particular document. \n",
    "\n",
    "This model represented words as atomic units, assuming that all words were independent of each other. It had success in several fields such as document classification, spam detection, and even sentiment analysis, but its assumptions (that words are completely independent of each other) were too strong for more powerful and accurate models. Enter Word2Vec. \n",
    "\n",
    "### The Skip-Gram and Continuous Bag of Words Models\n",
    "\n",
    "There are actually two different implementations of models that learn dense representation of words: the Skip-Gram model and the Continuous Bag of Words model. Both of these models learn dense vector representation of words, based on the words that surround them (ie, their *context*). \n",
    "\n",
    "The difference is that the skip-gram model predicts context (surrounding) words given the current word, wheras the continuous bag of words model predicts the current word based on several surrounding words. \n",
    "\n",
    "This notion of \"surrounding\" words is best described by considering a center (or current) word and a window of words around it. For example, if we consider the sentence \"The quick brown fox jumped over the lazy dog\", and a window size of 2, we'd have the following pairs for the skip-gram model:\n",
    "\n",
    "![img](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "In contrast, for the CBOW model, we'll input the context words within the window (such as \"the\", \"brown\", \"fox\") and aim to predict the target word \"quick\" (simply reversing the input -> prediction pipeline from the skip-gram model). \n",
    "\n",
    "The following is a visualization of the skip-gram and CBOW models:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/rohan-varma/paper-analysis/master/word2vec-papers/models.png)\n",
    "\n",
    "In this [paper](https://arxiv.org/pdf/1301.3781.pdf), the overall recommendation was to use the skip-gram model, since it had been shown to perform better on analogy-related tasks than the CBOW model. Overall, if you understand one model, it is pretty easy to understand the other: just reverse the inputs and predictions. Since both papers focused on the skip-gram model, this post will do the same. \n",
    "\n",
    "### Learning with the Skip-Gram Model\n",
    "\n",
    "Our goal is to find word representations that are useful for predicting the surrounding words given a current word. \n",
    "In particular, we wish to maximize the average log probability across our entire corpus: \n",
    "\n",
    "$$ argmax_{\\theta} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j \\in c, j != 0} log  p(w_{t + j} | w_{t} ; \\theta) $$\n",
    "\n",
    "This equation essentially says that there is some probability $p$ of observing a particular word that's within a window of size $c$ of the current word $w_t$. This probability is conditioned on the current word ($w_t$) and some setting of parameters $\\theta$ (determined by our model). We wish to set these parameters $\\theta$ so that this probability is maximized across our entire corpus.\n",
    "\n",
    "### Basic Parametrization: Softmax Model\n",
    "\n",
    "The basic skip-gram model defines $p(w_{t + j} | w_t)$ by the softmax function. If we consider $w_{t + j}$ to be a one-hot encoded vector with dimension $N$ and $\\theta$ to be a $N * K$ matrix embedding matrix (here, we have $N$ words in our vocabulary and our learned embeddings have dimension $K$), then we can define $p(w_{t + j} | w_t ; \\theta) = \\frac{exp(\\theta w_{t + j})}{\\sum_t exp(\\theta w_t)}$. It is worth noting that after learning, the matrix $\\theta$ can be thought of as an embedding lookup matrix. If you have a word that is represented with the $k$th index of a vector being hot, then the learning embedding for that word will be the $k$th column. \n",
    "\n",
    "This parametrization has a major disadvantage that limits its usefulness in cases of very large corpuses. Specifically, we notice that in order to compute a single forward pass of our model, we must sum across the entire corpus vocabulary in order to evaluate the softmax function. This is prohibitively expensive on large datasets, so we look to alternate approximations of this model for the sake of computational efficiency. \n",
    "\n",
    "\n",
    "### Hierarchical Softmax and Negative Sampling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
